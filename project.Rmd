---
title: "Project 2"
author: "Luke Terry"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    csl: biomed-central.csl
    df_print: paged
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    toc: yes
    toc_depth: 4
bibliography: project.bib
abstract: This project is all about applications of SLR to real data using R
---

<center>

![Luke Terry](me.jpg "My Picture"){ width=20% }

</center>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(dplyr)
```

# My Video

```{r}
# <video width="320" height="240" controls>
#   <source src="usingvideoinrmd.mp4" type="video/mp4">
# Your browser does not support the video tag.
# </video>
```




# Introduction

Baseball is a timeless sport which was popularized in the United States in the mid 1850s (@Wikipedia2021). Like many other sports in the United States, watching baseball, talking about baseball, and making predictions about baseball is a popular pastime among many people.

Compared to other sports, baseball and statistics have a close relationship. Perhaps it's something about the leisurely pace of a baseball game, or the easily measurable individual actions in each baseball play. Many people believe that the use of statistics in baseball was introduced by Billy Beane, the General Manager of the Oakland A's, since this narrative was popularized in Michael Lewis's 2003 book *Moneyball* (@Moneyball), and 2011 movie of the same name. Statistics in baseball, however, have been a consideration within coaching staff far before Billy Beane. There exist evidence of statisticians assisting baseball managers as early as 1940, and many general managers used statistical analysis to inform their decisions prior to Beane, such as Danny Evans of the Dodgers and Doug Melvin of the Brewers. Beane's fascination with OBP was not a new development either. Sandy Alderson rebuilt the A's strategy around OBP two decades before Billy Beane ever managed the A's (@NumbersGame).

There even exists a special term for the art of analyzing baseball statistics: sabermetrics. It was coined by Bill James, and influential statistician in the field, and its name refers to SABR, and acronym for the society of american baseball research (@NumbersGame)

## What are the variables? 

Each experimental unit in this case is one pitcher, or rather how one pitcher performed over the course of the 2019 season.

There are several statistics which are calculated based on each pitchers performance. These include traditional statistics, like Earned Run Average (ERA), Wins, and Losses, but I've also added a column for the hits a pitcher gives up per out they earn.

Each player is identified in the table by a playerID which is unique to them (believe it or not, there are several MLB players with the same name as other players throughout history).

The variables which I am interested in are any quantitative statistics of a pitcher, and the amount of hits per out they give up, a quantitative measure. I intend to find out which statistics (if any) predict the rate at which a pitcher gives up hits.

```{r pitcherTable}
library(Lahman)

ptall <- Lahman::Pitching
pt <- subset(ptall, ptall$yearID == 2019) #subset the data so that only 2019 data is analyzed

pt$HPO <- pt$H/pt$IPouts #add hits per out column

library(DT)
datatable(subset(pt, select = c("playerID", "teamID", "ERA", "W", "L", "HPO")))

#add statcast data to pt --------------------------------------------------------

stc <- read.csv("data/statcast_pitching_data.csv")
#for some reason different baseball databases use different playerIDS
#There was no direct crosswalk between the Lahman database and Statcast, so I had to make it myself.
#The script which does this is in the idCrosswalkTable.R file.
cw <- readRDS("data/crosswalk.RDS")

#initialize vector for new column
velocity <- c()
spin_rate <- c()
release_extension <- c()
xba <- c()
eff_min_vel <- c()
babip <- c()
takes <- c()
launch_angle <- c()

#add statcast data to Lahman pitcher data frame.
for (i in pt$playerID){
  #iterate over every pitcher, get their stats
  mlbID <- cw[which(cw == i),2]
  playerStats <- stc[which(stc$player_id == mlbID),]
  
  #add stats to column vectors
  velocity <- c(velocity, playerStats$velocity)
  spin_rate <- c(spin_rate, playerStats$spin_rate)
  release_extension <- c(release_extension, playerStats$release_extension)
  xba <- c(xba, playerStats$xba)
  eff_min_vel <- c(eff_min_vel, playerStats$eff_min_vel)
  babip <- c(babip, playerStats$babip)
  takes <- c(takes, playerStats$takes)
  launch_angle <- c(launch_angle, playerStats$launch_angle)

}

#add new columnds to pt
pt <- cbind(pt, velocity)
pt <- cbind(pt, spin_rate)
pt <- cbind(pt, release_extension)
pt <- cbind(pt, xba)
pt <- cbind(pt, eff_min_vel)
pt <- cbind(pt, babip)
pt <- cbind(pt, takes)
pt <- cbind(pt, launch_angle)


ptmin30 <- subset(pt, pt$G >= 30) #remove inexperienced or one-off pitchers for preliminary plots

```

### Plot data

```{r carcharacteristics, fig.height = 5, fig.cap = "MTCARS",fig.align='center',fig.cap="Graph of data with loess smoother"}
# library(ggplot2)
# g = ggplot(mtcars, aes(x = disp, y = mpg, color = cyl)) + geom_point()
# g = g + geom_smooth(method = "loess")
# g
```


## How were the data collected? 

Luckily for me, baseball's extra close relationship with statistical scrutiny means that copious amounts of reliable sabermetric data are available from various sources online, free of charge.

The specific data which I am using is a nearly exhaustive record of traditional baseball statistics compiled by Sean Lahman, called the Lahman Database(@Lahman). The data set contains standard batting and pitching statistics, for every player, by year, since 1871, although for the sake of brevity, my analysis will focus on the 2019 regular MLB season. Specifically, I intend to use linear regression to isolate factors which affect the rate at which pitchers give up hits.

The database was compiled from various sources by a team of volunteers, since it dates back to 1871. The season that I will focus on however (2019), was likely scraped from mlb.com since major league baseball now offers statistics in that online format.

The data itself, however, was collected by a combination of volunteers and mlb staff. 

## Why was it gathered? 

At its core, the data here was gathered to help mlb coaches and team administrations make informed decisions. The ubiquity and accuracy of sabermetrics, however, have allowed the media to take a real interest in baseball statistics.

## What is your interest in the data?

My personal interest in the data does not, however, lie in making informed decisions or predictions of overall baseball games. Each year, the MLB runs a competition called Beat the Streak. The rules are simple: each day, you are allowed to pick one or two batters who you think will get a hit that particular day. For each of your batters, if they get a hit, your streak goes up by 1. If they have an at bat, but fail to get a hit, your streak resets to 0. If you get a streak of 57 during the regular season, you win 5.6 million dollars (@bts). 

Obviously, this is a difficult challenge to achieve. The probability of winning it big follows a binomial distribution, with a probability of getting a hit equal to the probability that the chosen batter gets a hit in a day. The probability that a chosen batter gets a hit in a day ostensibly follows a binomial distribution, where the probabilty of a success in each Bernoulli trial is the players batting average, and the number of trials is the number of at bats the player is likely to get based on their position in the batting lineup. Below is a histogram of the seasonal batting averages for all major league players. To remove potential outliers, only players with more than 30 at bats in a season are included in this histogram.

```{r}
library(ggplot2)
bt <- Lahman::battingStats() #initialize batting data

#create histogram
bt <- subset(bt, bt$AB > 30)
g = ggplot(bt, aes(x=BA))
g = g + geom_histogram(fill = "#a62919", binwidth = .1)
g = g + xlab("Batting Averages")
g = g + ylab("Frequency")
g = g + ggtitle("Distribution of Seasonal Major League Batting Averages")
g
```

As you can see, it will be difficult to predict with certainty that a player gets a hit in a day. The average batting average for this historical dataset is 0.239. You can pick players with high batting averages, of course, but those players may not always get many plate appearances, and their batting averages don't get *that* much higher. For instance, the all time high batting average in this data set is still only 0.512. (The batting average is from Rudy Penderton's 1996 season with the Red Sox)

If we assume that a player as good as Rudy Penderton can be chosen everyday (which is a generous assumption), and we assume that this player gets 4 plate appearances per game (which is the average amount that most places in the order will get, especially when players who have a higher batting average are chosen @PA), then the probability of a hit is about 94%^[This is because the probability for getting a hit that day follows a binomial distribution with 4 trials, and a probability of the batting average for each trial (0.512 in Penderton's case). The R command used to find this answer is 1-dbinom(0,4,0.512)].

Although this is a high probability, we have to keep in mind that we don't just want to predict whether a player will get a hit in a day, we want to predict whether a player will get a hit in a day *57 days in a row*. Thus if we were able to pick Rudy Penderton every day, the probability of attaining a 57 day streak is 0.00203, or about 1 in 500 ^[This is because the probability of choosing 57 correct days in a row can be estimated with the geometric distribution, with 1- ~94% (or ~6%) as the probability, and 57 as the number of failures before 1 "success" (or in this case day without a hit) occurs. The R command used to find this answer is dgeom(57,1-pb), where pb is an object containint the exact probability of Penderton getting a hit in a day as calculated in the previous paragraph.].

This answer, however, assumes that each hit is a totally independent trial, where the only factor at play is the player's batting average. In reality, there are many factors which affect a baseball players ability to get hits. Conventional knowledge dictates that the pitcher which a batter is facing is the most important factor that dictates a player's ability to get a hit (apart from the players themselves). Because of this, this analysis will focus on what factors may cause a pitcher to give up more hits.

### Include pictures `![](jpeg)`

# Premliminary Plots and Interpretation of the data

In order to decide which variables to analyze using linear regression, it's useful to make preliminary plots and analyze them subjectively for seemingly linear patterns. Below are a few which I've constructed using some traditional pitching statistics. All graphs are a plot of a traditional pitching statistic versus a pitcher's Hits Per Out (HPO).

```{r echo = FALSE, warning=FALSE}
library(ggplot2)
library(patchwork)

createLowessForHPO <- function(column, xvar){
  g <- ggplot(ptmin30, aes(x = column, y = HPO)) + geom_point()
  g = g + geom_smooth(method = "loess")
  g = g + ggtitle(paste(xvar ," versus Hits given up Per Out"))
  g = g + ggtitle(xvar)
  g = g + xlab(xvar) + ylab("Hits Per Out")
  g
}


a = createLowessForHPO(ptmin30$ERA, "Earned Run Average")
b = createLowessForHPO(ptmin30$xba, "Expected Batting Average")
c = createLowessForHPO(ptmin30$babip, "Batting Average for Balls in Play")

a
b
c
```


```{r eval = FALSE, echo = FALSE}
library(shiny)
library(ggplot2)
library(patchwork)

#only render these plots if in an interactive document type
#if (interactive()){
  
ui <- basicPage(
  plotOutput("LoessPlot"),

  #list input variables
  selectInput(inputId = "x", label = "X Variable:", choices = c("Wins" = "W","Earned Runs" = "ER", "Earned Run Average" = "ERA", "Opponents Batting Average" = "BAOpp", "Total Walks Given Up" = "BB", "Strikeouts" = "SO", "Earned Runs" = "ER", "Velocity" = "velocity", "Spin Rate" = "spin_rate", "Release Extension" = "release_extension","Expected Batting Average" = "xba", "Effective Minimum Velocity" = "eff_min_vel", "Batting Average on Balls in Play" = "babip", "Hits Per Out" = "HPO","Takes" = "takes", "Launch Angle" = "launch_angle", "Hits" = "H")),
  
  selectInput(inputId = "y", label = "Y Variable:", choices = c("Wins" = "W","Earned Runs" = "ER", "Earned Run Average" = "ERA", "Opponents Batting Average" = "BAOpp", "Total Walks Given Up" = "BB", "Strikeouts" = "SO", "Earned Runs" = "ER", "Velocity" = "velocity", "Spin Rate" = "spin_rate", "Release Extension" = "release_extension","Expected Batting Average" = "xba", "Effective Minimum Velocity" = "eff_min_vel", "Batting Average on Balls in Play" = "babip", "Hits Per Out" = "HPO", "Takes" = "takes", "Launch Angle" = "launch_angle", "Hits" = "H")),

#input for minimum number of pitches
 sliderInput("mp", "Minimum number of Games for each pitcher",
    min = 0, max = 50, value = 30, step = 1)
  
)

server <- function(input, output) {
  output$LoessPlot <- renderPlot({
    
  #subset pt based on minimum number of pitches
  pt <- subset(pt, pt$G >= input$mp)
    
  g <- ggplot(data = pt, aes_string(x = input$x, y = input$y)) + geom_point()
  g = g + ggtitle(input$x)
  g = g + geom_smooth(method = "loess")

  g
  })

}

shinyApp(ui, server)

#}
```

For the sake of the preliminary analysis, I've filtered the pitching data to only include pitchers who've pitched a total of 30 games over their whole career. Heuristically, this allows me to minimize the effect on outliers in the data, although this is an oversimplification, and may exclude legitimate data. This is an acceptable sacrifice to make for the preliminary plots, however, since later in this analysis I'll take a more data-driven approach to eliminating outliers. Right now, I'm just looking for any kind of association.

As you can see, these three charts have vaguely linear patterns within them. Earned Run Average (ERA) refers to the amount of runs that a pitcher has, on average, given up per nine innings pitched. Expected batting average and Batting Average for Balls in Play are relatively new metrics to baseball, introduced through the Statcast system installed in all MLB ballparks in 2015 (@CITATIONNEEDED). Expected batting average refers to the likelihood that a batted ball becomes a hit, including home runs. Batting Average for Balls in Play (BABIP), however, refers to the liklihood that a batted ball in play (excluding home runs) becomes a hit. 

Of these three, I think that ERA is the most logical to analyze, given my goal to predict hits per inning given up. The other two are similar to the expected batting average against a hitter which, like Hits Per Out, is a statistic base on the amount of hits a pitcher gives up. 

Although ERA is affected by hits (hits earn runs), the two aren't directly tied together, since it's possible to get a hit (or several hits), without giving up any runs. There could theoretically exist pitchers who give up many hits, but give up few bases and thus not many runs. Additionally, there could be pitchers who give up very few hits, but give up large amounts of bases, and especially home runs, and thus have a very high ERA but a low HPO.

# Theory needed to carry out SLR

It is my belief that a pitcher's HPO tends to increase as their ERA does. Thus, when choosing between pitchers, a pitcher with a higher ERA will tend to give up more hits. I would like to create a model which relates the rate at which a pitcher gives up hits (HPO) to the pitchers, ERA, so I can use the more ubiquitous ERA to predict which pitchers are likely to give up more hits.

As with many sources of real data, however, an unknown number of factors and randomness affects a pitcher's ability to throw pitches, even if we assume that the skill of the pitcher does not change (or changes in a predictable career-aging curve (@CITATIONNEEDED)). In other words, in the preliminary plots, there seemed to be a vaguely linear association, but all of the points did *not* line up in a straight line. Because of this, a deterministic model cannot be used, and instead I will need to construct a probabilistic model.

One type of probabilistic model is simple linear regression (SLR). SLR assumes that there is a linear association between the variables, and uses data driven methods to construct a line which can be used to make predictions about y given x and a few constants. In the context of this analysis, we will assume that a pitcher's HPO is a function of their ERA, so our $Y$ (dependent) variable is HPO, and our $X$ (independent) variable is ERA. When the natural randomness of a pitcher is involved we must also add some amount of error $\epsilon$. Thus,

$$
Y_i = \beta_0 + \beta_1X_i + \epsilon_i
$$
where $i$ refers to a specific value of our independent variable (ERA). The regression line $Y_i = \beta_0 + \beta_1X_i$ is the expected value of $Y_i$ given $X_i$ since the epsilon error term can be assumed to be a normal random variable with $\mu = 0$ and $\sigma_\epsilon$ (@SLR). Although there is an independent and dependendent variable, this model does not imply that X causes Y, or vice versa. This analysis simply looks for an association between the two variables, not a causal relationship. This line of best fit is built on a variety of assumptions about the probability distribution of $\epsilon$, since it is needed to estimate the parameters $\beta_0$ and $\beta_1$. These assumptions are as follows (@MS):

 - $\mu_\epsilon = 0$; the mean of $\epsilon$ is 0
 - $V(\epsilon|X_i) = \sigma^2_\epsilon$; the variance of $\epsilon$ does not depend on $X$. No matter the value of X, the variance of $\epsilon$ is constant.
 - $\epsilon \sim N$; $\epsilon$ is normally distributed.
 - $\epsilon$ at any two observations are independent. Thus, the error at one observation has no effect on the errors at other observations.


# Estimating Parameters

In order to estimate the parameters $/beta_0$ and $\beta_1$, I will now use the Method of Least Squares. This involves finding the line in which the deviations from the expected value of the model (the regression line) are minimized. If all of the assumptions about $\epsilon$ which I outlined previously are true, the Method of Least Squares produces a line which is identical to the Method of Maximum Likelihood (@MS). Since the population least squares line is represented by $Y_i = \beta_0 + \beta_1X_1 + \epsilon_i$, my predicted line base on the 2019 MLB season (my sample) will be $\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}X_i + \epsilon_i$. For each $X_i$, I am able to calculate a residual $\hat{\epsilon}$, which is the deviation of the data point from my least squares regression line.
$$
 \hat\epsilon =  (y_i - \hat{y_i}) = (y_i - \hat{\beta_0} + \hat{\beta_1}x_i)
$$
Therefore, the least squares regression line will be the one which minimizes the sum of the squares for error (SSE). The SSE is given by

$$
\text{SSE} = \sum\limits_{i = 1}^n \hat{\epsilon}^2=  \sum\limits_{i = 1}^n (y_i - \hat{\beta_0} + \hat{\beta_1}x_i)^2
$$
I am searching for the least-squares estimates of the population parameters $\beta_0$ and $\beta_1$ which minimize the SSE. I expect that when I find these values, the residuals will be normally distributed with a mean of 0 and a constant variance.

```{r}
hpo.lm <- with(pt, lm(ERA ~ HPO))
sm <- summary(hpo.lm)
betas <- sm$coefficients
rsq <- sm$r.squared
```

The values predicted by this linear model are $\hat{\beta_0} = `r round(betas[1],4)`$ and $\hat{\beta_1} = `r round(betas[2],4)`$.
This model also has an $r^2$ value of $`r round(rsq, 4)`$. $r^2$, or the Coefficient of Determination, is a measure of the proportion of the variation in the population is accounted for in your model (@MS). It will become important further in this analysis, when we analyze whether this linear model is a useful predictor of a pitcher's HPO.

# Check for outliers

```{r}
g <- ggplot(data = pt, aes(x = ERA, y = HPO))
g = g + geom_point()
g = g + stat_smooth(method = "lm", col = "red")
g
```

As you can see, the plot of our data with the regression line looks significantly different than our preliminary plot. This is because I removed all pitchers who had pitched less than 30 full games worth of innings. At this point, however, I would like to take a more mathematical approach to removing outliers, in order to create the most accurate linear model that I can. This dataset lends itself to outliers since baseball position players sometimes pitch an inning or two in certain situations.

To find these outliers, I will use a metric called Cook's distance. This is a way of quantifying the influence that a data point has. Cook's distance takes into account the size of each datum's residual, as well as the leverage that a data point has on the regression line (@Cooks). 

```{r}
cd <- cooks.distance(hpo.lm)
cooksdf <- as.data.frame(cd)
cooksdf$obs <- c(1:length(cd))

cp <- ggplot(cooksdf, aes(x = obs, y = cd))
cp = cp + geom_col()
cp = cp + xlim(0, length(pt$playerID) + 5) #+ ylim(0, 1)
cp = cp + ggtitle("Cooks Distance Plot") + xlab("Observation Number") + ylab("Cook's Distance")
cp
```

As you can see from the plot, there are a few observations which have an excessively large effect on the regression. I will remove any datum which has a Cook's Distance larger than 0.004. The reason I chose 0.004, is because heuristically, a cook's distance over $\frac{1}{n}$, where $n$ is the number of observations, is too high (@statology). Since I have 930 data points, $\frac{1}{n} \approx 0.004$ As you can see, observation 893 exceeds this by a colossal margin. This observation is Aaron Wilkerson, a reliever who only pitched 16 innings for the Brewers during the 2019 season. As a reliever only briefly pulled up from the minor leagues, Wilkerson perfectly exemplifies why this data set is prone to outliers. Wilkerson was thrown into difficult pitching situations, very sporadically, and didn't seem to have the skill to hang in the major leagues. 

Because Aaron Wilkerson's gigantic Cook's distance throws off the scale of the Cook's plot, below is one where some data isn't shown, but it shows that there are multiple data points with a Cook's Distance over 0.004. 

```{r}
cd <- cooks.distance(hpo.lm)
cooksdf <- as.data.frame(cd)
cooksdf$obs <- c(1:length(cd))

cp <- ggplot(cooksdf, aes(x = obs, y = cd))
cp = cp + geom_col()
cp = cp + xlim(0, length(pt$playerID) + 5) + ylim(0, 0.3)
cp = cp + ggtitle("Cooks Distance Plot") + xlab("Observation Number") + ylab("Cook's Distance")
cp = cp + geom_hline(yintercept = 0.004, col = "red")
cp
```

# Development of model without Outliers

```{r}
#remove data with an excessively high Cook's Distance
baddata <- subset(cooksdf, cd >= 0.004, select = obs)$obs
ptf <- pt[-c(baddata),]

hpof.lm <- with(ptf, lm(ERA ~ HPO))
smf <- summary(hpof.lm)
betas <- smf$coefficients
rsq <- smf$r.squared
rsq
sm$r.squared
```

In the above block of code, I have produced another linear model, this time with the potential outliers removed. The values predicted by this linear model are $\hat{\beta_0} = `r round(betas[1],4)`$ and $\hat{\beta_1} = `r round(betas[2],4)`$.
This model also has an $r^2$ value of $`r round(rsq, 4)`$. $r^2$.

```{r}
g <- ggplot(data = ptf, aes(x = ERA, y = HPO))
g = g + geom_point()
g = g + stat_smooth(method = "lm", col = "red")
g
```

As you can see, some data points have been removed as outliers. That data point near the edge was not removed despite having high leverage. This is because it is so close to the line that it has a very low residual.

The higher $r^2$ value of this cleaned data indicates that it's a better fit for the data.

# Validity with mathematical expressions

Now that I have an estimate for the values in my linear model, it's important to analyze whether the assumptions that this was built on are true. This section will focus on the theory behind verifying assumptions; the next section will focus on actually verifying the assumptions in this data set.

If you recall an earlier section, there are four assumptions on which my model is built. These assumptions involve the distribution of the error, specifically that it is normally distributed, with a mean of zero, and a constant variance. Additionally, all errors should be independent of all other errors. To verify these assumptions, I will first use the residuals to find the Residual Sum of Squares (RSS). It is calculated using the following equation, where $\hat{y}$ is a value predicted by our regression line, and each $y_i$ is a data point.

$$
\text{RSS} = \sum\limits_{i=1}^n (\hat{y}_i - y_i)^2
$$

After this, I will find the Model Sum of Squares (MSS), which is the sum of the distances between the model's predicted value and the mean of the dependent variable at each data point, squared.

$$
\text{MSS} = \sum\limits_{i=1}^n (\hat{y}_i - \overline{Y})^2
$$

The third metric I would like to find is the Total Sum of Squares (TSS), which is the sum of the squared differences between each data point and the mean of the data.

$$
\text{TSS} = \sum\limits_{i=1}^n (y_i - \overline{Y})^2
$$

In order to better explain these variables and their relationship. The $R^2$ value which I mentioned earlier is calculated as $\frac{\text{MSS}}{\text{TSS}}$. 

After this analysis is complete, I intend to plot the residuals to show that there is no obvious pattern in them. Then, I will use the Shapiro-Wilk test for normality to show that the errors are normally distributed. 

The following function was taken from [https://rpubs.com/therimalaya/43190](https://rpubs.com/therimalaya/43190)

# Actual tests for Validity

In this section, I will put the theory introduced in the last section to work on our data set. 

```{r}
d <- ggplot(data = ptf, aes(x = HPO, y = ERA))
d = d + geom_point()
d = d + ggtitle("ERA vs HPO for 2019 MLB Pitchers")
d
```


### Straight trend line  

#### Use trendscatter

### Errors distributed Normally

$$\epsilon_i \sim N(0,\sigma^2)$$



#### Shapiro-wilk

### Constant variance

#### Residual vs fitted values

#### trendscatter on Residual Vs Fitted

### Zero mean value of $\epsilon$

### Independence of data 

# Model selection if you compared models 

## Use adjusted $R^2$ 
$$R_{adj}^2 =$$



# Analysis of the data

## Make sure you include many great plots

## Add the trend to the data


## Summary lm object

### Interpretation of all tests
### Interpretation of multiple R squared
### Interpretation of all point estimates
## Calculate cis for $\beta$ parameter estimates
### Use of `predict()`
### Use of `ciReg()`

### Check on outliers using cooks plots

Remember to interpret this plot and all other plots



# Conclusion
## Answer your research question
## Suggest ways to improve model or experiment


# References
  
